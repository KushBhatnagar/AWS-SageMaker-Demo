{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [**Introduction**](#Introduction)\n",
    "- [**Initiate SageMaker**](#InitiateSageMaker)\n",
    "- [**Download & Processing the Dataset**](#DownloadingandProcessingtheDataset)\n",
    "- [**Create SageMaker Experiment**](#SageMakerExperiment)\n",
    "- [**Evaluate the Model Performance**](#EvaluateModelPerformance)\n",
    "- [**Deploy the Model**](#DeploytheModel)\n",
    "- [**Monitor the Model**](#MonitortheModelEndpoint)\n",
    "- [**Summary**](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how Cloud Service - **AWS SageMaker** helps us to reduce time to production for Machine Learning projects. Amazon SageMaker provides a fully integrated development environment (IDE) called **SageMaker Studio**  for Machine Learning that provides a single, web-based visual interface to perform all the steps for ML development.\n",
    "\n",
    "We have used SageMaker Studio to build, train, deploy, and monitor an XGBoost model. We have covered the entire machine learning (ML) workflow from feature engineering and model training to batch and live deployments for ML models.\n",
    "\n",
    "The model will be trained on the [UCI Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) dataset that contains information on customer demographics, payment history, and billing history.\n",
    "\n",
    ">**This notebook is for tutorial purpose only and primarily demonstrate screenshots of steps taken during execution in SageMaker Studio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initiate SageMaker <a name=\"InitiateSageMaker\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open AWS console and create an AWS account, navigate to AWS SageMaker\n",
    "\n",
    "2. In the Amazon SageMaker navigation pane, choose SageMaker Studio\n",
    "\n",
    "3. In the Get started box, choose Quick Start and specify a user name\n",
    "\n",
    "4. For Execution role, choose Create an IAM role. In the dialog box that appears, choose Any S3 bucket and choose Create role and click submit.Amazon SageMaker creates a role with the required permissions and assigns it to your instance.\n",
    "\n",
    "> **The resources created and used in this notebook are AWS Free Tier eligible**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Processing the Dataset <a name=\"DownloadingandProcessingtheDataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Studio notebooks are one-click Jupyter notebooks that contain everything you need to build and test your training scripts. SageMaker Studio also includes experiment tracking and visualization so that itâ€™s easy to manage your entire machine learning workflow in one place.\n",
    "\n",
    "In the following steps we will create a SageMaker Notebook, download the dataset, and then upload the dataset to Amazon S3\n",
    "\n",
    "\n",
    "```Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** In the Amazon SageMaker Studio Control Panel, choose Open Studio.\n",
    "\n",
    "**2.** In JupyterLab, on the File menu, choose New, then Notebook. In the Select Kernel box, choose Python 3 (Data Science)\n",
    "<img src=\"images/2.2.png\">\n",
    "\n",
    "**3.** Verify version of the Amazon SageMaker Python SDK.Execute the following code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Installing previous SageMaker Version and restarting the kernel\")\n",
    "    !{sys.executable} -m pip install sagemaker==1.72.0\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Import required libraries and SageMaker Experiments by running following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments \n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Define the Amazon S3 buckets and folders for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawbucket= sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-modelmonitor' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Download the dataset and import it using the pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
    "data = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** We will use Amazon SageMaker's built-in **XGBoost algorithm** for modeling, but before importing required libraries for that we need to rename the last column as **Label** and extract the label column separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.7.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** Now upload the CSV dataset into an Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is uploaded in S3 bucket\n",
    "<img src=\"images/2.8_S3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.** After uploading the data, we will preprocessing the dataset which includes scaling the columns and splitting the dataset into train and test data. SageMaker lets us run our preprocessing, postprocessing, and model evaluation workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Amazon SageMaker Processing runs on separate compute instances from notebook instance. This means we can continue to experiment and run code in our notebook while the processing job is under way. This will incur additional charges for the cost of the instance which is up and running for the duration of the processing job. The instances are automatically terminated by SageMaker once the processing job completes```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Data Processing task we have to import the scikit-learn processing container. We have written **writefile preprocessing.py** script for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/2.9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.** Now, we will move this preprocessing code to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"images/2.10.png\">\n",
    "Code is uploaded in S3 bucket\n",
    "<img src =\"images/2.10_S3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.** Specify Training and Testing data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket+'/'+testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.** Now Process the data by SageMaker Processing Job. Following code will call **sklearn_processor.run** and extracts some optional metadata about the processing job, such as where the training and test outputs were stored. In below code snippet, we can see that locations of the code, train and test data in the outputs provided to the processor. Also, note the arguments provided to the processing scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code will take some time to execute and following will be the console output\n",
    "<img src =\"images/2.12.png\">\n",
    "After the execution following folders will be created in S3 bucket\n",
    "<img src =\"images/2.12_S3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Experiment <a name=\"SageMakerExperiment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** After downloading and staging the dataset in Amazon S3, we will create an **SageMaker Experiment**. An experiment is a collection of processing and training jobs related to the same machine learning project. SageMaker Experiments is a capability of Amazon SageMaker that lets you **organize, track, compare, and evaluate** your machine learning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"Build-train-deploy-{int(time.time())}\", \n",
    "    description=\"Predict credit card default from payments data\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(cc_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every training job is logged as a trial. Each trial is an iteration of our end-to-end training job. In addition to the training job, it can also track pre-processing and post-processing jobs as well as datasets and other metadata. A single experiment can include multiple trials which makes it easy for us to track multiple iterations over time within the Amazon SageMaker Studio Experiments pane. \n",
    "\n",
    "```This feature enables works done in one project to be utilized in another project,this is one of the advantage of using cloud service during POC phase```\n",
    "\n",
    "<img src=\"images/3.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Following code will track our pre-processing job under Experiments as well as a step in the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train_test_split_ratio\": 0.2,\n",
    "        \"random_state\":0\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view details of the experiment, Navigate to SageMaker Resources and select option `Experiments and trials` as SageMaker resources. Right click on the listed Trial Component and select `Open in trail component list` option\n",
    "<img src=\"images/3.2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** To train an **XGBoost classifier**, we need to first import the XGBoost container maintained by Amazon SageMaker. Then, we will log the training run under a Trial so SageMaker Experiments can track it under a Trial name. The pre-processing job is included under the same trial name since it is part of the pipeline. Next, we will create a SageMaker Estimator object, which automatically provisions the underlying instance type of our choosing, copies over the training data from the specified output location from the processing job, trains the model, and outputs the model artifacts.Following code will do the needful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-default-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    train_max_run=86400,\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    sagemaker_session=sess) # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs = {'train':s3_input_train},\n",
    "       job_name=cc_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following will be the console output\n",
    "<img src =\"images/3.3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** In the Experiment, a tab opens the listed Trial Component by clicking on the `Open in trail component list` option. The job will be listed as completed in the `Trial and Component List` tab\n",
    "<img src=\"images/3.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** To explore the associated metadata with the training job, right-click on one of the completed `Training jobs` and choose `Describe Trial Component`. This Page needs to be refreshed to see the latest results.\n",
    "\n",
    "<img src=\"images/3.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Performance <a name=\"EvaluateModelPerformance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** To evaluate the model performance on unseen data we will create **offline/batch inference** from the trained model.Refer [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) in Amazon documentation.\n",
    "\n",
    "Following code will copy the dataset over from the Amazon S3 location into our local folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 's3://' + test_data_location + '/test_data.csv'\n",
    "! aws s3 cp $test_data_path ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/4.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** After downloading the data, check the dataset along with all columns and extract the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/4.2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_full['0'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** To create the Batch Transform job we will run the following code. Like the training job, SageMaker provisions all the underlying resources, copies over the trained model artifacts, sets up a Batch endpoint locally, copies over the data, and runs inferences on the data and pushes the outputs to Amazon S3. Note that by setting the input_filter, we are letting Batch Transform know to neglect the first column in the test data which is the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\n",
    "sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** To evaluate the model metrics we will run the following code.Here, we have defined a function that pulls the output of the Batch Transform job, which is contained in a file with a **.out** extension from the Amazon S3 bucket and then we have extracted the predicted labels into a dataframe and append the true labels to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)\n",
    "output_df['Predicted']=np.round(output_df.values)\n",
    "output_df['Label'] = label\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosole output will be like this, which shows the total number of **Predicted** True and False values compared to the **Actual** values.  \n",
    "<img src=\"images/4.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** After model metrics evaluation, we will use the following code to extract baseline model accuracy and the model accuracy.\n",
    "\n",
    "```A helpful model for the baseline accuracy can be the fraction of non-default cases. A model that always predicts that a user will not default has that accuracy.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosole output will be like this.The results show that a simple model can already beat the baseline accuracy. In order to improve the results, we can tune the hyperparametersby using **Hyperparameter optimization (HPO)** on SageMaker for automatic model tuning\n",
    "<img src=\"images/4.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> ```At this point typical POC will be ended and based on the model evaluation results stakeholders will decide to deploy into production. If we executed above steps in a local notebook then it will take some time to move model/code and dataset for deployment which eventually increase time to production. Since we have used Cloud infrastructure there is no need to move data and model/code for deployment which means time to prodcution is reduced.```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model <a name=\"DeploytheModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Amazon SageMaker automatically handles the model hosting and creation of the endpoint, with the helpof below code we will deploy the model as a **RESTful HTTPS** endpoint to serve live inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(MaxResults=1,\n",
    "                                                SortBy='CreationTime',\n",
    "                                                SortOrder='Descending')\n",
    "\n",
    "training_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# create a model.\n",
    "def create_model(role, model_name, container_uri, model_data):\n",
    "    return sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_uri,\n",
    "        'ModelDataUrl': model_data,\n",
    "        },\n",
    "        ExecutionRoleArn=role)\n",
    "    \n",
    "\n",
    "try:\n",
    "    model = create_model(role, training_job_name, container_uri, model_data)\n",
    "except Exception as e:\n",
    "        sm_client.delete_model(ModelName=training_job_name)\n",
    "        model = create_model(role, training_job_name, container_uri, model_data)\n",
    "        \n",
    "\n",
    "print('Model created: '+model['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosole Output will be\n",
    "<img src=\"images/5.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Following code will specify data configuration settings SageMaker for captureing 100% of the inference payloads received by the endpoint, capture both inputs and outputs, and also note the input content type as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True,\n",
    "    \"InitialSamplingPercentage\": 100,\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path,\n",
    "    \"CaptureOptions\": [\n",
    "        { \"CaptureMode\": \"Output\" },\n",
    "        { \"CaptureMode\": \"Input\" }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"],\n",
    "       \"JsonContentTypes\": [\"application/json\"]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** The following code will create endpoint configuration and deploys the endpoint. Here, we have specified instance type and traffic allocation to this endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_config(model_config, data_capture_config): \n",
    "    return sm_client.create_endpoint_config(\n",
    "                                                EndpointConfigName=model_config,\n",
    "                                                ProductionVariants=[\n",
    "                                                        {\n",
    "                                                            'VariantName': 'AllTraffic',\n",
    "                                                            'ModelName': model_config,\n",
    "                                                            'InitialInstanceCount': 1,\n",
    "                                                            'InstanceType': 'ml.m4.xlarge',\n",
    "                                                            'InitialVariantWeight': 1.0,\n",
    "                                                },\n",
    "                                                    \n",
    "                                                    ],\n",
    "                                                DataCaptureConfig=data_capture_config\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "\n",
    "print('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5.3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** After Endpoint configuration, we need to create an actual endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\n",
    "\n",
    "endpoint_name = training_job_name\n",
    "def create_endpoint(endpoint_name, config_name):\n",
    "    return sm_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=training_job_name\n",
    "                                )\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "\n",
    "print('Endpoint created: '+ endpoint['EndpointArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Navigate to SageMaker Resources and select option `Endpoints` as SageMaker resources. The **Endpoints** list displays all of the endpoints in service.Once the endpoint is created, the status changes to **InService**\n",
    "<img src=\"images/5.5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** We will take first 10 rows as a sample of test dataset in `test_sample.csv` and send inference requests from this sample dataset to our endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 test_data.csv > test_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload[2:])\n",
    "        sleep(0.5)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** The following code will verify that Model Monitor is correctly capturing the incoming data.In the code, the `current_endpoint_capture_prefix` captures the directory path where our ModelMonitor outputs are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the captured json files.\n",
    "data_capture_prefix = '{}/monitoring'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "print(current_endpoint_capture_prefix)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "capture_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The console output indicates that data capture is configured and saving the incoming requests\n",
    "<img src=\"images/5.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** Now we will extract the content of one of the json files and view the captured outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[0])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Console output indicates that data capture is capturing both the input payload and the output of the model. \n",
    "<img src=\"images/5.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the Model Endpoint <a name=\"MonitortheModelEndpoint\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Now we will enable **SageMaker Model Monitor** to monitor the deployed endpoint for data drift. To do so we will compare the payload and outputs sent to the model against a baseline and determine whether there is any drift in the input data, or the label.\n",
    "Following code will create two folders: one folder stores the baseline data which will be used to train our model; the second folder stores any violations from that baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = prefix + \"/\" + endpoint_name\n",
    "baseline_prefix = model_prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\n",
    "train_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "print(train_data_header_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/6.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** The following code will set up a baseline job for Model Monitor to capture the statistics of the training data. To do this, Model Monitor uses the **deequ** library built on top of Apache Spark for conducting unit tests on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600)\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Monitor sets up a separate instance, copies over the training data, and generates some statistics. The service generates a lot of Apache Spark logs, which can be ignored. Once the job is completed, we can see **Spark job completed** output.\n",
    "<img src=\"images/6.2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Following code will check the output generated by the baseline job. It will create two files - **constraints.json** and **statistics.json**. This will converts the json output in these files into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of **statistics.json**\n",
    "<img src=\"images/6.3_statistics.png\">\n",
    "Content of **constraints.json**\n",
    "<img src=\"images/6.3_constraint.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** We can set up the frequency for endpoint monitoring. Here we have specifies an hourly frequency, but it can be changed  for production applications as hourly frequency will generate a lot of data. Model Monitor will produce a report consisting of all the violations it finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\n",
    "print(s3_report_path)\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this code enables Amazon CloudWatch Metrics, which instructs Model Monitor to send outputs to CloudWatch. You can use this approach to trigger alarms using CloudWatch Alarms to let engineers or admins know when data drift has been detected.\n",
    "<img src=\"images/6.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary <a name=\"Summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the following as a part of POC and Deployment. \n",
    " \n",
    "- Download and Preprocessing the Data\n",
    "- Create reusable Experiment Trials with model building\n",
    "- Evaluate Model Performance \n",
    "- Deploy and monitor the Model\n",
    "\n",
    "The first three are mainly part of POC and the last one belongs to Deployment. And all these activities are done within a single notebook without bothering about additional resources and infrastructure. Amazon SageMaker manages an S3 bucket for storage and separates compute instances for processing\n",
    "\n",
    "For this demo we have used the `ml.t3.medium` instance and for one-day usage, it cost us around **1.2 USD** only which is very **cost-effective**.Refer  [this](https://aws.amazon.com/sagemaker/pricing/) link for AWS resources pricing.\n",
    "\n",
    "<img src=\"images/pricing.png\">\n",
    "\n",
    "Like XGBoost, SageMaker provides the container and libraries for other industry-standard algorithms, and this will come with plenty of documentation which means the **short learning curve** We have referred AWS official [tutorial](https://aws.amazon.com/getting-started/hands-on/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=gs_card) for this demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered the following advantages of using Cloud service right from the POC phase\n",
    "***\n",
    "- **Reduced time to production**\n",
    "- **Cost-Effective**\n",
    "- **Avoid Local infrastructure**\n",
    "- **Shorter Learning Curve**\n",
    "- **Early adoption of MLOPs**\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
